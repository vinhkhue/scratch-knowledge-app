#!/usr/bin/env python3
"""
Script t·∫°o parquet files phong ph√∫ cho Scratch Knowledge Graph
T·∫°o nhi·ªÅu entities v√† relationships ƒë·ªÉ ƒë·ªì th·ªã ƒë·∫πp h∆°n
"""
import pandas as pd
import json
from pathlib import Path
import hashlib
import re
import openai
import os
import logging
from typing import List, Dict, Any

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize OpenAI client
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in environment variables")

client = openai.OpenAI(api_key=OPENAI_API_KEY)


def extract_block_entities_from_wiki(content: str) -> List[Dict[str, Any]]:
    """Tr√≠ch xu·∫•t c√°c kh·ªëi l·ªánh t·ª´ Scratch Wiki (Blocks.txt)"""
    
    system_prompt = """Tr√≠ch xu·∫•t c√°c kh·ªëi l·ªánh Scratch t·ª´ n·ªôi dung. Tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y (kh√¥ng c√≥ markdown):
{"entities": [{"name": "t√™n kh·ªëi", "description": "ch·ª©c nƒÉng", "type": "block", "category": "Motion", "importance": "high"}]}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Tr√≠ch xu·∫•t 20-30 kh·ªëi l·ªánh t·ª´:\n{content[:2000]}"}
            ],
            temperature=0.2,
            max_tokens=3000
        )
        
        # Fix: Strip markdown formatting
        raw_response = response.choices[0].message.content
        if raw_response.startswith('```json'):
            raw_response = raw_response[7:]  # Remove ```json
        if raw_response.endswith('```'):
            raw_response = raw_response[:-3]  # Remove ```
        raw_response = raw_response.strip()
        
        result = json.loads(raw_response)
        entities = result.get('entities', [])
        logger.info(f"‚úÖ Tr√≠ch xu·∫•t {len(entities)} block entities t·ª´ Scratch Wiki")
        return entities
    except Exception as e:
        logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t blocks: {e}")
        try:
            raw_response = response.choices[0].message.content
            logger.error(f"Raw response: {raw_response[:200]}...")
        except:
            pass
        return []

def extract_educational_entities(content: str, source: str) -> List[Dict[str, Any]]:
    """Tr√≠ch xu·∫•t kh√°i ni·ªám gi√°o d·ª•c t·ª´ Tin h·ªçc 8"""
    
    system_prompt = """Tr√≠ch xu·∫•t c√°c kh√°i ni·ªám Scratch t·ª´ n·ªôi dung. Tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y (kh√¥ng c√≥ markdown):
{"entities": [{"name": "t√™n", "description": "m√¥ t·∫£", "type": "concept", "category": "basic", "importance": "high"}]}"""

    chunk_size = 1500  # Chia nh·ªè h∆°n
    overlap = 300
    all_entities = []
    
    for i in range(0, len(content), chunk_size - overlap):
        chunk = content[i:i + chunk_size]
        chunk_num = i // (chunk_size - overlap) + 1
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Tr√≠ch xu·∫•t 10-15 entities t·ª´:\n{chunk[:1000]}"}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            # Fix: Strip markdown formatting
            raw_response = response.choices[0].message.content
            if raw_response.startswith('```json'):
                raw_response = raw_response[7:]  # Remove ```json
            if raw_response.endswith('```'):
                raw_response = raw_response[:-3]  # Remove ```
            raw_response = raw_response.strip()
            
            result = json.loads(raw_response)
            entities = result.get('entities', [])
            logger.info(f"  Chunk {chunk_num}: {len(entities)} entities")
            all_entities.extend(entities)
        except Exception as e:
            logger.error(f"Error chunk {chunk_num}: {e}")
            # Debug raw response
            try:
                raw_response = response.choices[0].message.content
                logger.error(f"Raw response: {raw_response[:200]}...")
            except:
                pass
    
    # Remove duplicates
    unique = {}
    for e in all_entities:
        if e['name'] not in unique:
            unique[e['name']] = e
    
    logger.info(f"‚úÖ Tr√≠ch xu·∫•t {len(unique)} educational entities")
    return list(unique.values())

def extract_cross_file_relationships(block_entities: List[Dict], edu_entities: List[Dict], content: str) -> List[Dict[str, Any]]:
    """T·∫°o relationships g·∫Øn k·∫øt gi·ªØa blocks v√† educational concepts"""
    
    block_names = [e['name'] for e in block_entities[:30]]
    edu_names = [e['name'] for e in edu_entities[:20]]
    
    system_prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch m·ªëi quan h·ªá gi·ªØa kh·ªëi l·ªánh Scratch v√† kh√°i ni·ªám gi√°o d·ª•c.

Block entities (t·ª´ Scratch Wiki): {', '.join(block_names[:20])}
Educational concepts (t·ª´ Tin h·ªçc 8): {', '.join(edu_names[:15])}

H√£y t√¨m T·ªêI THI·ªÇU 50-60 relationships g·∫Øn k·∫øt 2 ngu·ªìn, bao g·ªìm:
1. Block thu·ªôc category n√†o (Move Steps thu·ªôc Motion)
2. Block ƒë∆∞·ª£c d√πng ƒë·ªÉ l√†m g√¨ (Move Steps ƒë·ªÉ di chuy·ªÉn sprite)
3. Concept ƒë∆∞·ª£c implement b·∫±ng block n√†o (v√≤ng l·∫∑p d√πng Forever, Repeat)
4. Block li√™n quan ƒë·∫øn thu·ªôc t√≠nh n√†o (Set X to li√™n quan ƒë·∫øn v·ªã tr√≠)

JSON format (tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y, kh√¥ng c√≥ markdown):
{{
    "relationships": [
        {{
            "source": "entity ngu·ªìn",
            "target": "entity ƒë√≠ch",
            "relation": "m√¥ t·∫£ m·ªëi quan h·ªá",
            "type": "belongs_to|implements|controls|relates_to",
            "strength": "strong|medium|weak"
        }}
    ]
}}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"T·∫°o relationships:\n\n{content[:2000]}"}
            ],
            temperature=0.3,
            max_tokens=3000
        )
        
        # Fix: Strip markdown formatting
        raw_response = response.choices[0].message.content
        if raw_response.startswith('```json'):
            raw_response = raw_response[7:]  # Remove ```json
        if raw_response.endswith('```'):
            raw_response = raw_response[:-3]  # Remove ```
        raw_response = raw_response.strip()
        
        result = json.loads(raw_response)
        relationships = result.get('relationships', [])
        logger.info(f"‚úÖ T·∫°o {len(relationships)} cross-file relationships")
        return relationships
    except Exception as e:
        logger.error(f"‚ùå L·ªói t·∫°o relationships: {e}")
        return []

def find_entity_by_name(entities_df, name):
    
    # Th·ª≠ exact match tr∆∞·ªõc
    exact_match = entities_df[entities_df['title'].str.lower() == name.lower()]
    if not exact_match.empty:
        return exact_match.iloc[0]['id']
    
    # Th·ª≠ contains match
    contains_match = entities_df[entities_df['title'].str.contains(name, case=False, na=False, regex=False)]
    if not contains_match.empty:
        return contains_match.iloc[0]['id']
    
    # Th·ª≠ partial match v·ªõi c√°c t·ª´ kh√≥a ch√≠nh (t·ª´ > 3 k√Ω t·ª±)
    keywords = name.split()
    for keyword in keywords:
        if len(keyword) > 3:
            partial_match = entities_df[entities_df['title'].str.contains(keyword, case=False, na=False, regex=False)]
            if not partial_match.empty:
                return partial_match.iloc[0]['id']
    
    return None

def create_rich_scratch_parquet_files():
    """T·∫°o c√°c parquet files phong ph√∫ cho GraphRAG"""
    
    # ƒê·ªçc t·∫•t c·∫£ file .txt v√† ph√¢n lo·∫°i
    input_dir = Path("input")
    txt_files = list(input_dir.glob("*.txt"))
    
    if not txt_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file .txt n√†o trong th∆∞ m·ª•c input")
        return
    
    print(f"üìÅ T√¨m th·∫•y {len(txt_files)} file .txt:")
    for file in txt_files:
        print(f"  - {file.name}")
    
    all_content = []
    file_sources = {}
    all_llm_entities = []
    block_entities = []
    edu_entities = []
    
    for file in txt_files:
        try:
            content = file.read_text(encoding='utf-8')
            
            if "Blocks" in file.name:
                source = "Scratch Wiki"
            elif "tin-hoc-8" in file.name or "scratch_clean" in file.name:
                source = "Tin h·ªçc 8 - C√°nh di·ªÅu"
            else:
                source = "Unknown"
            
            file_sources[file.name] = source
            all_content.append(f"\n\n=== {file.name} ({source}) ===\n\n{content}")
            print(f"‚úÖ ƒê√£ ƒë·ªçc {file.name} ({source}) - {len(content)} k√Ω t·ª±")
            
            if "Blocks" in file.name:
                # File Scratch Wiki - tr√≠ch xu·∫•t blocks
                logger.info(f"ü§ñ Tr√≠ch xu·∫•t blocks t·ª´ {file.name}...")
                blocks = extract_block_entities_from_wiki(content)
                block_entities.extend(blocks)
                all_llm_entities.extend(blocks)
            else:
                # File Tin h·ªçc 8 - tr√≠ch xu·∫•t educational concepts
                logger.info(f"ü§ñ Tr√≠ch xu·∫•t educational concepts t·ª´ {file.name}...")
                edu = extract_educational_entities(content, source)
                edu_entities.extend(edu)
                all_llm_entities.extend(edu)
                
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc {file.name}: {e}")
    
    content = "\n".join(all_content)
    
    # T·∫°o relationships g·∫Øn k·∫øt
    logger.info("ü§ñ T·∫°o cross-file relationships...")
    llm_relationships = extract_cross_file_relationships(block_entities, edu_entities, content)
    
    print(f"\nüìä T·ªïng k·∫øt tr√≠ch xu·∫•t LLM:")
    print(f"  - Block entities: {len(block_entities)}")
    print(f"  - Educational entities: {len(edu_entities)}")
    print(f"  - Total entities: {len(all_llm_entities)}")
    print(f"  - Relationships: {len(llm_relationships)}")
    
    # T·∫°o documents.parquet
    documents_data = {
        'id': ['doc_1'],
        'text': [content],
        'title': ['Scratch Knowledge Base - Tin h·ªçc 8 + Scratch Wiki'],
        'creation_date': ['2024-01-01T00:00:00Z'],
        'metadata': [{
            'source': 'mixed', 
            'subject': 'scratch',
            'files': list(file_sources.keys()),
            'sources': list(file_sources.values())
        }]
    }
    documents_df = pd.DataFrame(documents_data)
    documents_df.to_parquet('output/documents.parquet', index=False)
    print("‚úÖ Created documents.parquet")
    
    # T·∫°o text_units.parquet (chunk text)
    chunks = []
    chunk_size = 1000
    overlap = 200
    
    for i in range(0, len(content), chunk_size - overlap):
        chunk_text = content[i:i + chunk_size]
        chunk_id = f"tu_{i//chunk_size + 1}"
        
        chunks.append({
            'id': chunk_id,
            'text': chunk_text,
            'title': f'Chunk {i//chunk_size + 1}',
            'document_id': 'doc_1',
            'metadata': {'chunk_index': i//chunk_size + 1, 'length': len(chunk_text)}
        })
    
    text_units_df = pd.DataFrame(chunks)
    text_units_df.to_parquet('output/text_units.parquet', index=False)
    print(f"‚úÖ Created text_units.parquet with {len(chunks)} chunks")
    
    # T·∫°o entities.parquet t·ª´ LLM extraction
    entities_data = []
    entity_id = 1
    
    for entity in all_llm_entities:
        entities_data.append({
            'id': f'e_{entity_id}',
            'title': entity['name'],
            'description': entity['description'],
            'type': entity.get('type', 'concept'),
            'text_unit_ids': [f'tu_{j+1}' for j in range(min(3, len(chunks)))],
            'metadata': {
                'category': entity.get('category', 'unknown'),
                'source': 'Scratch Wiki' if entity.get('type') == 'block' else 'Tin h·ªçc 8',
                'importance': entity.get('importance', 'medium'),
                'extracted_by': 'llm'
            }
        })
        entity_id += 1
    
    # Fallback n·∫øu LLM kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c g√¨
    if not entities_data:
        print("‚ö†Ô∏è LLM kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c entities, s·ª≠ d·ª•ng d·ªØ li·ªáu fallback...")
        scratch_concepts_fallback = {
        'Scratch': 'Ng√¥n ng·ªØ l·∫≠p tr√¨nh tr·ª±c quan',
        'l·∫≠p tr√¨nh': 'Qu√° tr√¨nh t·∫°o ra ch∆∞∆°ng tr√¨nh m√°y t√≠nh',
        'kh·ªëi l·ªánh': 'C√°c th√†nh ph·∫ßn c∆° b·∫£n ƒë·ªÉ t·∫°o ch∆∞∆°ng tr√¨nh',
        'sprite': 'Nh√¢n v·∫≠t ho·∫°t ƒë·ªông trong Scratch',
        's√¢n kh·∫•u': 'Kh√¥ng gian l√†m vi·ªác ch√≠nh',
            'stage': 'S√¢n kh·∫•u trong Scratch'
        }
        
        for concept, description in scratch_concepts_fallback.items():
            entities_data.append({
                'id': f'e_{entity_id}',
                'title': concept,
                'description': description,
                'type': 'concept',
                'text_unit_ids': [f'tu_{i+1}' for i in range(min(3, len(chunks)))],
                'metadata': {
                    'category': 'fallback',
                    'source': 'manual',
                    'importance': 'high' if concept in ['Scratch', 'sprite', 'kh·ªëi l·ªánh', 's√¢n kh·∫•u'] else 'medium',
                    'extracted_by': 'fallback'
                }
            })
            entity_id += 1
    
    entities_df = pd.DataFrame(entities_data)
    entities_df.to_parquet('output/entities.parquet', index=False)
    print(f"‚úÖ Created entities.parquet with {len(entities_data)} entities")
    
    # T·∫°o relationships.parquet t·ª´ LLM extraction
    relationships_data = []
    rel_id = 1
    
    for rel in llm_relationships:
        # S·ª≠ d·ª•ng fuzzy matching ƒë·ªÉ t√¨m entities
        source_id = find_entity_by_name(entities_df, rel['source'])
        target_id = find_entity_by_name(entities_df, rel['target'])
        
        if source_id and target_id:
            relationships_data.append({
                'id': f'r_{rel_id}',
                'source': source_id,
                'target': target_id,
                'description': rel['relation'],
                'weight': 0.8,
                'text_unit_ids': [f'tu_{i+1}' for i in range(min(3, len(chunks)))],
                'metadata': {
                    'relation_type': rel.get('type', 'unknown'),
                    'strength': rel.get('strength', 'medium'),
                    'extracted_by': 'llm'
                }
            })
            rel_id += 1
        else:
            # Log relationships that couldn't be matched
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y entity cho relationship: {rel['source']} -> {rel['target']}")
    
    # Fallback relationships n·∫øu LLM kh√¥ng t·∫°o ƒë∆∞·ª£c
    if not relationships_data:
        print("‚ö†Ô∏è LLM kh√¥ng t·∫°o ƒë∆∞·ª£c relationships, s·ª≠ d·ª•ng d·ªØ li·ªáu fallback...")
        rich_relations_fallback = [
            ('Scratch', 'l·∫≠p tr√¨nh', 'Scratch ƒë∆∞·ª£c d√πng ƒë·ªÉ l·∫≠p tr√¨nh'),
            ('sprite', 's√¢n kh·∫•u', 'Sprite ho·∫°t ƒë·ªông tr√™n s√¢n kh·∫•u')
        ]
        
        for source, target, description in rich_relations_fallback:
            source_entities = entities_df[entities_df['title'].str.contains(source, case=False, na=False)]
            target_entities = entities_df[entities_df['title'].str.contains(target, case=False, na=False)]
            
            if not source_entities.empty and not target_entities.empty:
                relationships_data.append({
                    'id': f'r_{rel_id}',
                    'source': source_entities.iloc[0]['id'],
                    'target': target_entities.iloc[0]['id'],
                    'description': description,
                    'weight': 0.8,
                    'text_unit_ids': [f'tu_{i+1}' for i in range(min(3, len(chunks)))],
                    'metadata': {
                        'relation_type': 'conceptual',
                        'strength': 'strong',
                        'extracted_by': 'fallback'
                    }
                })
                rel_id += 1
    
    relationships_df = pd.DataFrame(relationships_data)
    relationships_df.to_parquet('output/relationships.parquet', index=False)
    print(f"‚úÖ Created relationships.parquet with {len(relationships_data)} relationships")
    
    # T·∫°o communities.parquet phong ph√∫ h∆°n
    communities_data = {
        'id': ['c_1', 'c_2', 'c_3', 'c_4'],
        'level': [1, 1, 2, 2],
        'entities': [
            [f'e_{i+1}' for i in range(min(10, len(entities_data)))],  # Basic concepts
            [f'e_{i+1}' for i in range(min(10, len(entities_data)), min(20, len(entities_data)))],  # Programming concepts
            [f'e_{i+1}' for i in range(min(20, len(entities_data)), min(30, len(entities_data)))],  # Advanced features
            [f'e_{i+1}' for i in range(min(30, len(entities_data)), len(entities_data))]  # Educational concepts
        ],
        'metadata': [
            {'name': 'Basic Concepts', 'type': 'basic', 'color': 'blue'},
            {'name': 'Programming Concepts', 'type': 'programming', 'color': 'green'},
            {'name': 'Advanced Features', 'type': 'advanced', 'color': 'orange'},
            {'name': 'Educational Concepts', 'type': 'educational', 'color': 'purple'}
        ]
    }
    communities_df = pd.DataFrame(communities_data)
    communities_df.to_parquet('output/communities.parquet', index=False)
    print("‚úÖ Created communities.parquet")
    
    # T·∫°o community_reports.parquet phong ph√∫
    community_reports_data = {
        'id': ['cr_1', 'cr_2', 'cr_3', 'cr_4'],
        'community_id': ['c_1', 'c_2', 'c_3', 'c_4'],
        'title': ['B√°o c√°o kh√°i ni·ªám c∆° b·∫£n', 'B√°o c√°o kh√°i ni·ªám l·∫≠p tr√¨nh', 'B√°o c√°o t√≠nh nƒÉng n√¢ng cao', 'B√°o c√°o kh√°i ni·ªám gi√°o d·ª•c'],
        'summary': [
            'T√≥m t·∫Øt c√°c kh√°i ni·ªám c∆° b·∫£n nh·∫•t trong Scratch nh∆∞ sprite, s√¢n kh·∫•u, kh·ªëi l·ªánh.',
            'T√≥m t·∫Øt c√°c kh√°i ni·ªám l·∫≠p tr√¨nh c·ªët l√µi nh∆∞ v√≤ng l·∫∑p, ƒëi·ªÅu ki·ªán, bi·∫øn, h√†m.',
            'T√≥m t·∫Øt c√°c t√≠nh nƒÉng n√¢ng cao nh∆∞ clone, pen, sensing, video, music.',
            'T√≥m t·∫Øt c√°c kh√°i ni·ªám gi√°o d·ª•c nh∆∞ t∆∞ duy logic, gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ, s√°ng t·∫°o.'
        ],
        'metadata': [
            {'report_type': 'summary', 'generated_by': 'script'},
            {'report_type': 'summary', 'generated_by': 'script'},
            {'report_type': 'summary', 'generated_by': 'script'},
            {'report_type': 'summary', 'generated_by': 'script'}
        ]
    }
    community_reports_df = pd.DataFrame(community_reports_data)
    community_reports_df.to_parquet('output/community_reports.parquet', index=False)
    print("‚úÖ Created community_reports.parquet")
    
    print("\nüéâ T·∫•t c·∫£ parquet files phong ph√∫ ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!")
    print("üìä Th·ªëng k√™:")
    print(f"  - Documents: {len(documents_df)}")
    print(f"  - Text Units: {len(text_units_df)}")
    print(f"  - Entities: {len(entities_df)}")
    print(f"  - Relationships: {len(relationships_df)}")
    print(f"  - Communities: {len(communities_df)}")
    print(f"  - Community Reports: {len(community_reports_df)}")
    
    # Hi·ªÉn th·ªã m·ªôt s·ªë entities v√† relationships m·∫´u
    print("\nüîç M·ªôt s·ªë entities m·∫´u:")
    for entity in entities_data[:10]:
        print(f"  - {entity['title']}: {entity['description']}")
    
    print("\nüîó M·ªôt s·ªë relationships m·∫´u:")
    for rel in relationships_data[:10]:
        print(f"  - {rel['description']}")

if __name__ == "__main__":
    create_rich_scratch_parquet_files()
